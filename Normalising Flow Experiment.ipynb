{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=7>Normalising Flow Experiments</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>This source code contains all the experiments we did throughout the project. The main important sections are the 1D, 2D, 3D and Image models. \n",
    "\n",
    "The 1D Model is located under the 'Experimental 1D Single Flow Model' heading \\\n",
    "The 2D Model is located under the '2D Flow-based Model Pipeline' heading\\\n",
    "The 3D Model is located under the '3D Flow-based Model Pipeline' heading\\\n",
    "The Image Model is located under the 'Image Flow-based Model Pipeline' heading\\\n",
    "\n",
    "Wherever there is \"Path\", Please input an appropriate path to save the loss history.\n",
    "<font size=5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Some generative model theory<font size=6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change of Variable Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $x = T(z)$, where $T$ is a Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to probability $\\int _{x}{P_x(X)dx} = \\int _{z}{P_z(Z)dz}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore $|P_x(X)dx| = |P_z(Z)dz|$\n",
    "\n",
    "$\\qquad \\qquad \\quad \\ \\ P_x(X) = P_z(Z)|\\frac{dz}{dx}|$\n",
    "\n",
    "$\\qquad \\qquad \\quad \\ \\ P_x(X) = P_z(Z)|\\frac{\\delta T^{-1}(x)}{\\delta x}|$\n",
    "\n",
    "$\\qquad \\qquad \\quad \\ \\ P_x(X) = P_z(T^{-1}(x))|detJ_{T^{-1}}(x)|$\n",
    "\n",
    "$\\qquad \\qquad \\quad \\ \\ P_x(X) = P_z(Z)|detJ_{T}(Z)|^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood Function for the Target Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In normal flow parlance, $p_Z(z;\\psi)$ is called the base distribution and $p_X(x)$ the target distribution. \n",
    "If $p_Z$ is a Gaussian, then $\\psi = (\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$\n",
    "\n",
    "$T$ depends on parameters $\\phi$. We can consider the likelihood of the data under the target distribution. Denoting the parameters by $\\theta$, the likelihood is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\boldsymbol{X} \\mid \\theta) = \\Pi^N_{i=1} P_x(\\boldsymbol{x}_i \\mid \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $N$ data points $\\boldsymbol{x}_i$, collectively denoted by $\\boldsymbol{X}$.\n",
    "\n",
    "For the log likelihood,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ln P(X|\\theta) = \\displaystyle \\sum^{N}_{i = 1}ln P_x(x_i|\\theta)$ \n",
    "\n",
    "$\\qquad \\qquad \\ \\ = \\displaystyle \\sum^{N}_{i = 1} ln |detJ_T^{-1}(x_i;\\phi)| + lnP_Z(T^{-1}(x_i;\\phi);\\psi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the base distribution is known and easy to sample from. Here, we take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P_Z(z) = \\mathcal{N}(\\mu,\\Sigma) = 2 \\pi^{-D/2}|{det}(\\Sigma)^{-\\frac{1}{2}}| \\exp \\{-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating loss by KL divergence and log likelihood, where n real data we can sample from $P^{*}(X)$, $P_{x}(X;\\theta)$ is a flow-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{L}(\\theta) = \\mathcal{D}_{KL}[P^{*}(X)||P_{x}(X;\\theta)]$\n",
    "\n",
    "$\\qquad \\ =-E_{P^{*}(X)}[\\log P_{x}(X;\\theta)] + const$\n",
    "\n",
    "$\\qquad \\ =-E_{P^{*}(X)}[\\log P_{z}(T^{-1}(x;\\theta)) + \\log|det J_{T^{-1}}(x;\\theta)|] + const$\n",
    "\n",
    "$\\qquad \\ \\approx -\\frac{1}{N} \\displaystyle \\sum^{n}_{i = 1} [\\log P_{z}(T^{-1}(x_i;\\theta)) + \\log|det J_{T^{-1}}(x_i;\\theta)|] + const$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>2D uniform transform</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "\n",
    "from torch import nn, optim\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_2D_graph(data,color=\"steelblue\",size=5):\n",
    "    fig = plt.figure(figsize = (3*6.4,1*6.4))\n",
    "    ax = plt.subplot(1,3,1)\n",
    "    plt.title(\"Total\")\n",
    "    ax = plt.scatter([i[0] for i in data],[i[1] for i in data],s=size,c=color)\n",
    "    ax = plt.subplot(1,3,2)\n",
    "    plt.title(\"X\")\n",
    "    ax = plt.hist([i[0] for i in data],20,density=True,color=color)\n",
    "    ax = plt.subplot(1,3,3)\n",
    "    plt.title(\"Y\")\n",
    "    ax = plt.hist([i[1] for i in data],20,density=True,color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_graph(data1,data2,title1,title2,color=\"steelblue\",size=7):\n",
    "    fig = plt.figure(figsize = (2*9.6,1*9.6))\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    plt.title(title1)\n",
    "    ax = plt.scatter(data1.T[0],data1.T[1],s=size,c=color)\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    plt.title(title2)\n",
    "    ax = plt.scatter(data2.T[0],data2.T[1],s=size,c=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_3D_graph(data1,data2,title1,title2):\n",
    "    fig = plt.figure(figsize=(2 * 9.6, 2 * 9.6))\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax.scatter3D(data1.T[0], data1.T[1], data1.T[2], c = np.abs(data1.T[0]) + np.abs(data1.T[1]) + np.abs(data1.T[2]), cmap =plt.get_cmap('rainbow_r'))\n",
    "    plt.title(title1)\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax.scatter3D(data2.T[0], data2.T[1], data2.T[2], c = np.abs(data2.T[0]) + np.abs(data2.T[1]) + np.abs(data2.T[2]), cmap =plt.get_cmap('rainbow_r'))\n",
    "    plt.title(title2)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_image(data1,data2,title1,title2):\n",
    "    fig = plt.figure(figsize = (2*9.6,1*9.6))\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    plt.title(title1)\n",
    "    plt.imshow(np.transpose(data1.detach().numpy(),(1,2,0)),cmap = 'gray')\n",
    "    ax = plt.subplot(1,2,2)\n",
    "    plt.title(title2)\n",
    "    plt.imshow(np.transpose(data2.detach().numpy(),(1,2,0)),cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_graph(path,pos):\n",
    "    data = torch.load(path)\n",
    "    history = data[\"stats\"]\n",
    "    fig = plt.figure(figsize = (3*6.4,1*6.4))\n",
    "    ax1 = plt.subplot()\n",
    "    ax1.plot(history[0][:pos], 'r', label='Training Loss')\n",
    "    ax1.plot(history[1][:pos], 'orange', label='Testing Loss')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.legend(loc='upper right')\n",
    "    plt.title('Training and Testing loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model,test_loader,loss_fn,target_distribution):\n",
    "    epoch_loss = []\n",
    "    with torch.no_grad():\n",
    "        for x in test_loader:\n",
    "            z, dz_by_dx = model(x)\n",
    "            loss = loss_fn(target_distribution,z,dz_by_dx)\n",
    "            epoch_loss.append(loss)\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate uniform distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = rng.uniform(-1,1,1000)\n",
    "y_vals = rng.uniform(-1,1,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine two arrays and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "for i in range(len(x_vals)):\n",
    "    vals.append([x_vals[i],y_vals[i]])\n",
    "\n",
    "draw_2D_graph(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the weight to transform the uniform distrbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [[2,1],[1,2]]\n",
    "trans_vals = np.dot(vals,W)\n",
    "\n",
    "draw_2D_graph(trans_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>2D Gaussian Transformation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 2D Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = rng.normal(0,1,1000)\n",
    "y_vals = rng.normal(0,1,1000)\n",
    "\n",
    "vals = []\n",
    "for i in range(len(x_vals)):\n",
    "    vals.append([x_vals[i],y_vals[i]])\n",
    "\n",
    "draw_2D_graph(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set transform function, where $s(x)y = 5x^2y$ and $b(x) = 2x$, therefore $z = s(x)y + b(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform (data):\n",
    "    data_trans = []\n",
    "\n",
    "    for data_point in data:\n",
    "        data_y = 5 * data_point[0] * data_point[0] * data_point[1] + 2 * data_point[0]\n",
    "        data_trans.append([data_point[0],data_y])\n",
    "\n",
    "    return data_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply function and draw the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = transform(vals)\n",
    "\n",
    "draw_2D_graph(result,\"darkcyan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set inverse transform function, where original $y = \\frac{s(x)-2x}{5x^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform (data):\n",
    "    data_inverse_trans = []\n",
    "\n",
    "    for data_point in data:\n",
    "        data_y = (data_point[1] - 2 * data_point[0]) / (5 * data_point[0] * data_point[0])\n",
    "        data_inverse_trans.append([data_point[0],data_y])\n",
    "\n",
    "    return data_inverse_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply function and draw the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_result = inverse_transform(result)\n",
    "\n",
    "draw_2D_graph(inverse_result,\"forestgreen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>SVD</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD ($A=UWV^T and\\ A^{-1}=VW^{-1}U^T $), $A$ in SVD represent the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set an original matrix $A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mat(\"1,2;0,1\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $A^{-1}$ calculating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_weight_matrix(m):\n",
    "    u,sigma,vt = np.linalg.svd(m,full_matrices=True)\n",
    "\n",
    "    s_inv = np.zeros([len(m),len(m)])\n",
    "    for i in range(len(m)):\n",
    "        s_inv[i][i] = 1/sigma[i]\n",
    "\n",
    "    return np.dot(vt.T,s_inv).dot(u.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate $A^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(inverse_weight_matrix(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Implementation of SVD</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = rng.normal(0,1,1000)\n",
    "y_vals = rng.normal(0,1,1000)\n",
    "\n",
    "vals = []\n",
    "for i in range(len(x_vals)):\n",
    "    vals.append([x_vals[i],y_vals[i]])\n",
    "\n",
    "draw_2D_graph(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion by matrix, i.e. $T(z) = 2x+3y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [\n",
    "    [1,0],\n",
    "    [2,3]\n",
    "]\n",
    "\n",
    "W = np.array(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the transform result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.dot(vals,W.T)\n",
    "\n",
    "draw_2D_graph(result,\"darkcyan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse the Matrix to calculate original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_inv = inverse_weight_matrix(W.T)\n",
    "\n",
    "draw_2D_graph(inverse_result,\"forestgreen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>FCL transform pipeline with KL Divergence</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the original data (Uniform Distribution) / Convert original data to tensor and flatten it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = rng.uniform(-1,1,1000)\n",
    "y_vals = rng.uniform(-1,1,1000)\n",
    "\n",
    "uniform = []\n",
    "for i in range(len(x_vals)):\n",
    "    uniform.append([x_vals[i],y_vals[i]])\n",
    "\n",
    "draw_2D_graph(uniform,size=10)\n",
    "\n",
    "input = torch.tensor(uniform) #input shape is (1000,2)\n",
    "input = nn.Flatten(0,-1)(input) #input shape is (2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert target data to tensor and flatten it / Draw the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor(trans_vals[:1000]) #target shape is (1000,2)\n",
    "target = nn.Flatten(0,-1)(target) #target shape is (2000)\n",
    "\n",
    "draw_2D_graph(trans_vals[:1000],size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a very simple model with one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = nn.Sequential(\n",
    "    nn.Linear(2000,2000),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, apply KL divergence to calculating loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.KLDivLoss(reduction='sum')\n",
    "optimizer = optim.Adam(flow.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = flow(input.float())\n",
    "    loss = loss_fn(output.softmax(dim=-1).log(), target.float().softmax(dim=-1))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Iter:{epoch+1}   KLDivLoss:{loss.item(): .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the original data to genterate output from the model / Draw the predict output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = flow(input.float())\n",
    "\n",
    "tmp_output = nn.Unflatten(0,(1000,2))(output)\n",
    "tmp_output = tmp_output.detach().numpy()\n",
    "\n",
    "draw_2D_graph(tmp_output,size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get parameters from the model, parameters are in matrix form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_matrix,bias = flow.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is actually calculated like $Input \\cdot ParametersMatrix = Output $ i.e. $[1\\times2000]\\cdot[2000\\times2000]=[1\\times2000]$, the parameters matrix is $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(input,parameter_matrix.detach().numpy().T) + bias.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying SVD to get inverse parameters matrix, which is the $f^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_matrix = inverse_weight_matrix(parameter_matrix.detach().numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply output as input into $f^{-1}$ and try to get original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_input = np.matmul((output.detach().numpy() - bias.detach().numpy()),inverse_matrix)\n",
    "predict_origin = torch.tensor(tmp_input).unflatten(0,(1000,2)).numpy()\n",
    "\n",
    "draw_2D_graph(predict_origin,size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Experimental 1D Single Flow Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_gaussian(number_of_points):\n",
    "    n = number_of_points // 2\n",
    "    gaussian_1 = np.random.normal(loc=-1,scale=0.25,size=(n,))\n",
    "    gaussian_2 = np.random.normal(loc=0.5,scale=0.5,size=(number_of_points - n,))\n",
    "    return np.concatenate([gaussian_1,gaussian_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,array):\n",
    "        super().__init__()\n",
    "        self.array = array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.array[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model by using CDF calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flow_1d(nn.Module):\n",
    "\n",
    "    def __init__(self,n_components):\n",
    "        super(flow_1d,self).__init__()\n",
    "        self.mus = nn.Parameter(torch.randn(n_components),requires_grad=True)\n",
    "        self.log_sigmas = nn.Parameter(torch.zeros(n_components),requires_grad=True)\n",
    "        self.weight_logits = nn.Parameter(torch.ones(n_components),requires_grad=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,1)\n",
    "        weights = self.weight_logits.softmax(dim=0).view(1,-1)\n",
    "        distribution = torch.distributions.Normal(self.mus,self.log_sigmas.exp())\n",
    "        z = (distribution.cdf(x) * weights).sum(dim=1)\n",
    "        dz_by_dx = (distribution.log_prob(x).exp() * weights).sum(dim=1)\n",
    "        return z,dz_by_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function by negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(target_distribution,z,dz_by_dx):\n",
    "    log_likelihood = target_distribution.log_prob(z) + dz_by_dx.log()\n",
    "    return -log_likelihood.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,test_loader,optimizer,target_distribution):\n",
    "    epoch = 100\n",
    "    history = np.zeros((2,epoch))\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        epoch_loss = []\n",
    "        for x in train_loader:\n",
    "            z, dz_by_dx = model(x)\n",
    "            loss = loss_fn(target_distribution,z,dz_by_dx)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_loss = np.mean(epoch_loss)\n",
    "            test_loss = validation(model,test_loader,loss_fn,target_distribution)\n",
    "            print(f\"Iter: {i+1} Train Loss: {train_loss: .3f} Test Loss: {test_loss: .3f}\")\n",
    "\n",
    "        history[:,i] = (train_loss,test_loss)\n",
    "\n",
    "    torch.save({\"stats\": history}, \"Path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = 2000, 2000\n",
    "train_data = mixture_gaussian(n_train)\n",
    "test_data = mixture_gaussian(n_test)\n",
    "\n",
    "train_loader = data.DataLoader(Dataset(train_data),batch_size=128,shuffle=True)\n",
    "test_loader = data.DataLoader(Dataset(test_data),batch_size=2000,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set target distribution which is Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = torch.distributions.Uniform(low=0,high=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the flow model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = flow_1d(n_components=3)\n",
    "\n",
    "optimizer = optim.Adam(flow.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(flow,train_loader,test_loader,optimizer,target_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_graph(\"Path\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw distribution of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (1*6.4,1*6.4))\n",
    "_ = plt.hist(train_data,bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw distribution of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (1*6.4,1*6.4))\n",
    "_ = plt.hist(test_data,bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw distribution of target (Uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (1*6.4,1*6.4))\n",
    "_ = plt.hist(target_distribution.sample([2000]).detach().numpy(),bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw generated distribution after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.eval()\n",
    "z, dz_by_dx = flow(next(iter(test_loader)))\n",
    "\n",
    "plt.figure(figsize = (1*6.4,1*6.4))\n",
    "_ = plt.hist(z.detach().numpy(),bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (1*6.4,1*6.4))\n",
    "_ = plt.hist(z.detach().numpy(),bins=50,label='Generated distribution')\n",
    "_ = plt.hist(target_distribution.sample([2000]).detach().numpy(),bins=50,color='darkorange',alpha=0.8,label='Target distribution')\n",
    "_ = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Single transform pipeline with log-likelihood 2-D</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_gaussian_2d(number_of_points):\n",
    "    n = number_of_points // 2\n",
    "    gaussian_1 = np.random.multivariate_normal(mean=[-2.,2.],cov=[[2.,-1.],[-1.,2.]],size=(n))\n",
    "    gaussian_2 = np.random.multivariate_normal(mean=[2.,-2.],cov=[[4.,2.],[2.,4.]],size=(number_of_points - n))\n",
    "    return np.concatenate([gaussian_1,gaussian_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,array):\n",
    "        super().__init__()\n",
    "        self.array = array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.array[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model by using polynomial funtion $s(x) \\times y+b(x)$ where $s(x)$ and $b(x)$ are 3rd polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flow_2d(nn.Module):\n",
    "\n",
    "    def __init__(self,n_components):\n",
    "        super(flow_2d,self).__init__()\n",
    "        self.n_components = n_components\n",
    "        self.phi = 0\n",
    "        self.sweights = nn.Parameter(torch.randn(n_components,dtype=torch.double).view(-1,1),requires_grad=True)\n",
    "        self.bweights = nn.Parameter(torch.randn(n_components,dtype=torch.double).view(-1,1),requires_grad=True)\n",
    "\n",
    "    def forward(self,X):\n",
    "        x = X.T[0].view(-1,1)\n",
    "        y = X.T[1].view(-1,1)\n",
    "\n",
    "        self.phi = x.T**0\n",
    "        for i in range(self.n_components-1):\n",
    "            self.phi = torch.vstack((self.phi,x.T**(i+1)))\n",
    "        self.phi = self.phi.T\n",
    "\n",
    "        s = torch.sigmoid(self.phi@self.sweights.view(-1,1))\n",
    "        b = torch.sigmoid(self.phi@self.bweights.view(-1,1))\n",
    "        y_new = s * y + b\n",
    "        \n",
    "        z = torch.vstack((x.T,y_new.T)).T\n",
    "\n",
    "        return z,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function by negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(target_distribution,z,log_dz_by_dx):\n",
    "    log_likelihood = target_distribution.log_prob(z).view(-1,1) + log_dz_by_dx\n",
    "    return -log_likelihood.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epoch,train_loader,test_loader,optimizer,target_distribution):\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        epoch_loss = []\n",
    "        for x in train_loader:\n",
    "            z, dz_by_dx = model(x)\n",
    "            loss = loss_fn(target_distribution,z,dz_by_dx)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_loss = np.mean(epoch_loss)\n",
    "            test_loss = validation(model,test_loader,loss_fn,target_distribution)\n",
    "            print(f\"Iter: {i+1} Train Loss: {train_loss: .3f} Test Loss: {test_loss: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test = 2000, 2000\n",
    "train_data = mixture_gaussian_2d(n_train)\n",
    "test_data = mixture_gaussian_2d(n_test)\n",
    "\n",
    "train_loader = data.DataLoader(Dataset(train_data),batch_size=256,shuffle=True)\n",
    "test_loader = data.DataLoader(Dataset(test_data),batch_size=n_test,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the flow model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = flow_2d(3)\n",
    "optimizer = optim.Adam(flow.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the target distribution which is Multivariate Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare original distribution and before trained model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_train = next(iter(test_loader))\n",
    "flow.eval()\n",
    "z,dz_by_dx = flow(before_train)\n",
    "\n",
    "compare_graph(test_data,z.detach().numpy(),\"P(X)\",\"Before Trained T(X)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(flow,1000,train_loader,optimizer,target_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare target distribution and generated distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_train = next(iter(test_loader))\n",
    "flow.eval()\n",
    "z,dz_by_dx = flow(after_train)\n",
    "\n",
    "compare_graph(target_distribution.sample([2000]).detach().numpy(),z.detach().numpy(),\"gaussian\",\"Trained T(X)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare original distribution and generated distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_graph(test_data,z.detach().numpy(),\"P(X)\",\"Learned P(Z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>2D Flow-based Model Pipeline</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion to generate original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_gaussian_2d(number_of_points):\n",
    "    n = number_of_points // 3\n",
    "    gaussian_1 = np.random.multivariate_normal(mean=[-2.,4.],cov=[[4.,-2.],[-2.,4.]],size=(n))\n",
    "    gaussian_2 = np.random.multivariate_normal(mean=[-1.,0.],cov=[[1.,0.],[0.,4.]],size=(n))\n",
    "    gaussian_3 = np.random.multivariate_normal(mean=[2.,2.],cov=[[4.,2.],[2.,4.]],size=(number_of_points - 2 * n))\n",
    "    return np.concatenate([gaussian_1,gaussian_2,gaussian_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,array):\n",
    "        super().__init__()\n",
    "        self.array = array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.array[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size, dtype=torch.double)]\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append( nn.Linear(hidden_size, hidden_size, dtype=torch.double) )\n",
    "            layers.append( nn.ReLU() )\n",
    "        layers.append( nn.Linear(hidden_size, output_size, dtype=torch.double) )\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class flow_2d(nn.Module):\n",
    "    def __init__(self, pos, hidden_size=128, num_hidden_layers=6):\n",
    "        super(flow_2d, self).__init__()\n",
    "        self.mlp = MLP(2, hidden_size, num_hidden_layers, 2)\n",
    "        if pos == 1: \n",
    "            self.mask = torch.tensor([1,0],dtype=torch.double) \n",
    "        else:\n",
    "            self.mask = torch.tensor([0,1],dtype=torch.double)\n",
    "        self.mask = self.mask.view(1,-1)\n",
    "        self.scale_weight = nn.Parameter(torch.zeros(1,dtype=torch.double), requires_grad=True)\n",
    "        self.bias_weight = nn.Parameter(torch.zeros(1,dtype=torch.double), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        x_masked = x * self.mask\n",
    "        log_scale, bias = self.mlp(x_masked).chunk(2, dim=1)\n",
    "        log_scale = log_scale.tanh() * self.scale_weight + self.bias_weight\n",
    "        bias = bias  * (1-self.mask)\n",
    "        log_scale = log_scale * (1-self.mask)\n",
    "        if reverse:\n",
    "            x = (x - bias) * torch.exp(-log_scale)\n",
    "        else:\n",
    "            x = x * torch.exp(log_scale) + bias\n",
    "        return x, log_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model to compose flows and pass results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compose_flow(nn.Module):\n",
    "    def __init__(self, flow_list):\n",
    "        super(compose_flow, self).__init__()\n",
    "        self.flow_list = nn.ModuleList(flow_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, log_det_jacobian = x, torch.zeros_like(x)\n",
    "        for flow in self.flow_list:\n",
    "            z, log_scale = flow(z)\n",
    "            log_det_jacobian += log_scale\n",
    "        return z, log_det_jacobian\n",
    "\n",
    "    def inverse(self, z):\n",
    "        list = []\n",
    "        for flow in self.flow_list[::-1]:\n",
    "            z, _ = flow(z, reverse=True)\n",
    "            list.append(z)\n",
    "        return list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function by negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(target_distribution,z,sum_log_dz_by_dx):\n",
    "    log_likelihood = target_distribution.log_prob(z).view(-1,1) + sum_log_dz_by_dx\n",
    "    return -log_likelihood.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epoch,train_loader,test_loader,target_distribution,lr=0.001):\n",
    "    optimizer = optim.AdamW(flows.parameters(),lr=lr)\n",
    "    model.train()\n",
    "    history = np.zeros((2,epoch))\n",
    "    for i in range(epoch):\n",
    "        epoch_loss = []\n",
    "        for x in train_loader:\n",
    "            z, sum_log_dz_by_dx = model(x)\n",
    "            loss = loss_fn(target_distribution,z,sum_log_dz_by_dx)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_loss = np.mean(epoch_loss)\n",
    "            test_loss = validation(model,test_loader,loss_fn,target_distribution)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Iter: {i+1} Train Loss: {train_loss: .3f} Test Loss: {test_loss: .3f}\")\n",
    "        history[:,i] = (train_loss,test_loss)\n",
    "\n",
    "    torch.save({\"stats\": history}, \"Path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mixture_gaussian_2d(1024)\n",
    "test_data = mixture_gaussian_2d(1024)\n",
    "\n",
    "train_loader = data.DataLoader(Dataset(train_data),batch_size=512,shuffle=True)\n",
    "test_loader = data.DataLoader(Dataset(test_data),batch_size=1024,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_graph(train_data,test_data,\"\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the flow model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_list = [flow_2d(1), flow_2d(2),flow_2d(1), flow_2d(2),flow_2d(1), flow_2d(2)]\n",
    "flows = compose_flow(flows_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the target distribution which is Multivariate Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare original distribution and before trained model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_train = next(iter(test_loader))\n",
    "flows.eval()\n",
    "z,dz_by_dx = flows(before_train)\n",
    "\n",
    "compare_graph(test_data,z.detach().numpy(),\"Original distribution\",\"Before trained generated distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(flows,2000,train_loader,test_loader,target_distribution,lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_graph(\"Path\",500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare target distribution and generated distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_train = next(iter(test_loader))\n",
    "flows.eval()\n",
    "z,dz_by_dx = flows(after_train)\n",
    "\n",
    "compare_graph(target_distribution.sample([1024]).detach().numpy(),z.detach().numpy(),\"Target distribution (Gaussian)\",\"Generated distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = target_distribution.sample([1024]).detach().numpy()\n",
    "g = z.detach().numpy()\n",
    "\n",
    "plt.figure(figsize = (1*9.6,1*9.6))\n",
    "_ = plt.scatter(t.T[0],t.T[1],s=7,c=\"darkorange\",label='Target distribution',alpha=0.6)\n",
    "_ = plt.scatter(g.T[0],g.T[1],s=7,label='Generated distribution')\n",
    "_ = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare original distribution and generated distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_graph(test_data,z.detach().numpy(),\"Original distribution\",\"Generated distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a graph showing every inverse flow output (Original Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = flows.inverse(z)\n",
    "fig = plt.figure(figsize = (4*4.8,3*4.8))\n",
    "generated_from_trainning_result = 0\n",
    "\n",
    "for f in range(len(ls)):\n",
    "    ax = plt.subplot(3,4,f+1)\n",
    "    plt.title(\"After {}th flow's inverse function\".format(len(ls) - f))\n",
    "\n",
    "    z_ = ls[f].detach().numpy()\n",
    "    generated_from_trainning_result = z_\n",
    "    x, y = z_.T[0], z_.T[1]\n",
    "    ax = plt.scatter(x,y,s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a graph showing every inverse flow output (Reverse Generated Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = target_distribution.sample([1024])\n",
    "ls = flows.inverse(g)\n",
    "fig = plt.figure(figsize = (4*4.8,3*4.8))\n",
    "generated_from_gaussian = 0\n",
    "\n",
    "for f in range(len(ls)):\n",
    "    ax = plt.subplot(3,4,f+1)\n",
    "    plt.title(\"After {}th flow's inverse function\".format(len(ls) - f))\n",
    "\n",
    "    z_ = ls[f].detach().numpy()\n",
    "    generated_from_gaussian = z_\n",
    "    x, y = z_.T[0], z_.T[1]\n",
    "    ax = plt.scatter(x,y,s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between orginal distribution and reverse generated distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = generated_from_trainning_result\n",
    "g = generated_from_gaussian\n",
    "\n",
    "plt.figure(figsize = (1*9.6,1*9.6))\n",
    "_ = plt.scatter(t.T[0],t.T[1],s=7,c=\"darkorange\",label='Original distribution',alpha=0.5)\n",
    "_ = plt.scatter(g.T[0],g.T[1],s=7,alpha=0.8,label='Generated from Gaussian sampling')\n",
    "_ = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Experimental 2D Model Pipeline with Mixture Inputs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta = torch.distributions.Beta(torch.tensor([1.,1.]),torch.tensor([1.,1.]))\n",
    "w1 = [[1.,2.],[2.,1.]]\n",
    "w2 = [[1.,2.],[2.,1.]]\n",
    "w3 = [[1.,2.],[-2.,1.]]\n",
    "\n",
    "dist1 = (Beta.sample([1000])@torch.tensor(w1)).detach().numpy()\n",
    "dist2 = (Beta.sample([1000])@torch.tensor(w2)@torch.tensor(w3)).detach().numpy()\n",
    "dist3 = (Beta.sample([1000])@torch.tensor(w1)).detach().numpy()\n",
    "dist4 = (Beta.sample([1000])@torch.tensor(w2)@torch.tensor(w3)).detach().numpy()\n",
    "\n",
    "dist1, dist2, dist3, dist4 = np.double(dist1), np.double(dist2), np.double(dist3), np.double(dist4)\n",
    "\n",
    "train_dist, test_dist = np.concatenate([dist1,dist2]), np.concatenate([dist3,dist4])\n",
    "\n",
    "draw_2D_graph(train_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,array):\n",
    "        super().__init__()\n",
    "        self.array = array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.array[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model by using polynomial funtion $s(x) \\times y+b(x)$ where $s(x)$ and $b(x)$ are 3rd polynomial and its inverse funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flow_2d(nn.Module):\n",
    "\n",
    "    def __init__(self,n_components,flows_number=1):\n",
    "        super(flow_2d,self).__init__()\n",
    "        self.n_components = n_components\n",
    "        self.flows_number = flows_number\n",
    "        self.phi = 0\n",
    "        self.sweights = nn.Parameter(torch.randn(n_components,dtype=torch.double).view(-1,1),requires_grad=True)\n",
    "        self.bweights = nn.Parameter(torch.randn(n_components,dtype=torch.double).view(-1,1),requires_grad=True)\n",
    "        self.w1_weights = nn.Parameter(torch.tensor(1,dtype=torch.double).view(-1,1),requires_grad=True)\n",
    "        self.w2_weights = nn.Parameter(torch.tensor(1,dtype=torch.double).view(-1,1),requires_grad=True)\n",
    "\n",
    "\n",
    "    def forward(self,X):\n",
    "        if self.flows_number % 2 == 1:\n",
    "            x = X.T[0].view(-1,1)\n",
    "            y = X.T[1].view(-1,1)\n",
    "        else:\n",
    "            x = X.T[1].view(-1,1)\n",
    "            y = X.T[0].view(-1,1)\n",
    "\n",
    "        self.phi = x.T**0\n",
    "        for i in range(self.n_components-1):\n",
    "            self.phi = torch.vstack((self.phi,x.T**(i+1)))\n",
    "        self.phi = self.phi.T\n",
    "\n",
    "        self.s = torch.sigmoid(self.phi@self.sweights.view(-1,1))\n",
    "        self.b = torch.sigmoid(self.phi@self.bweights.view(-1,1))\n",
    "        y_new = self.w1_weights * self.s * y + self.w2_weights * self.b\n",
    "        \n",
    "        if self.flows_number % 2 == 1:\n",
    "            z = torch.vstack((x.T,y_new.T)).T\n",
    "        else:\n",
    "            z = torch.vstack((y_new.T,x.T)).T\n",
    "\n",
    "        return z, (self.w1_weights * self.s).log()\n",
    "\n",
    "    def inverse(self,Z):\n",
    "        if self.flows_number % 2 == 1:\n",
    "            x = Z.T[0].view(-1,1)\n",
    "            y = Z.T[1].view(-1,1)\n",
    "        else:\n",
    "            x = Z.T[1].view(-1,1)\n",
    "            y = Z.T[0].view(-1,1)\n",
    "\n",
    "        y_old = (y - self.w2_weights * self.b) / (self.w1_weights * self.s)\n",
    "\n",
    "        if self.flows_number % 2 == 1:\n",
    "            z = torch.vstack((x.T,y_old.T)).T\n",
    "        else:\n",
    "            z = torch.vstack((y_old.T,x.T)).T\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model to compose flows and pass results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class composed_flows(nn.Module):\n",
    "    \n",
    "    def __init__(self,models_list):\n",
    "        super(composed_flows,self).__init__()\n",
    "        self.models_list = nn.ModuleList(models_list)\n",
    "\n",
    "    def forward(self,X):\n",
    "        z, sum_log_dz_by_dx = X, 0\n",
    "        for flow in self.models_list:\n",
    "            z, log_dz_by_dx = flow(z)\n",
    "            sum_log_dz_by_dx += log_dz_by_dx\n",
    "        \n",
    "        return z, sum_log_dz_by_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function by negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(target_distribution,z,sum_log_dz_by_dx):\n",
    "    log_likelihood = target_distribution.log_prob(z).view(-1,1) + sum_log_dz_by_dx\n",
    "    return -log_likelihood.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epoch,train_loader,optimizer,target_distribution):\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        epoch_loss = []\n",
    "        for x in train_loader:\n",
    "            z, sum_log_dz_by_dx = model(x)\n",
    "            loss = loss_fn(target_distribution,z,sum_log_dz_by_dx)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(\"Iter: {} Loss: {:.3f}\".format(i+1,np.mean(epoch_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_dist\n",
    "test_data = test_dist\n",
    "\n",
    "train_loader = data.DataLoader(Dataset(train_data),batch_size=1024,shuffle=True)\n",
    "test_loader = data.DataLoader(Dataset(test_data),batch_size=2000,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the flow list and initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_list = [flow_2d(3,1),flow_2d(3,2),flow_2d(3,3),flow_2d(3,4),flow_2d(3,5),flow_2d(3,6),flow_2d(3,7),flow_2d(3,8),flow_2d(3,9),flow_2d(3,10)]\n",
    "flows = composed_flows(flows_list)\n",
    "\n",
    "optimizer = optim.AdamW(flows.parameters(),lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the target distribution which is Multivariate Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = torch.distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare original distribution and before trained model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_train = next(iter(test_loader))\n",
    "flows.eval()\n",
    "z,dz_by_dx = flows(before_train)\n",
    "\n",
    "compare_graph(test_data,z.detach().numpy(),\"P(X)\",\"Before trained T(X)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(flows,10000,train_loader,optimizer,target_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare target distribution and generated distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_train = next(iter(test_loader))\n",
    "flows.eval()\n",
    "z,dz_by_dx = flows(after_train)\n",
    "\n",
    "compare_graph(target_distribution.sample([2000]).detach().numpy(),z.detach().numpy(),\"Target distribution (Gaussian)\",\"Trained T(X)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare original distribution and generated distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_graph(test_data,z.detach().numpy(),\"P(X)\",\"Trained T(X)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a graph for every inverse flow output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4*4.8,3*4.8))\n",
    "\n",
    "for f in range(len(flows_list)):\n",
    "    ax = plt.subplot(3,4,f+1)\n",
    "    plt.title(\"The {}th T^-1(Z)\".format(len(flows_list) - f))\n",
    "\n",
    "    z = flows_list[len(flows_list) - f - 1].inverse(z)\n",
    "    z_ = z.detach().numpy()\n",
    "    x, y = z_.T[0], z_.T[1]\n",
    "    ax = plt.scatter(x,y,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1_loader = data.DataLoader(Dataset(dist1),batch_size=1000,shuffle=True)\n",
    "dist2_loader = data.DataLoader(Dataset(dist2),batch_size=1000,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = next(iter(dist1_loader))\n",
    "flows.eval()\n",
    "z,dz_by_dx = flows(dist1)\n",
    "\n",
    "compare_graph(dist1,z.detach().numpy(),\"One part of distribution \",\"T(X) according to the part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2 = next(iter(dist2_loader))\n",
    "flows.eval()\n",
    "z,dz_by_dx = flows(dist2)\n",
    "\n",
    "compare_graph(dist2,z.detach().numpy(),\"Another part of distribution \",\"T(X) according to the part\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>3D Flow-based Model Pipeline</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-3.,3.,2048)\n",
    "y = np.random.uniform(-3.,3.,2048)\n",
    "z = -(x**2 + y**2) - np.random.uniform(0.,5.,2048)\n",
    "\n",
    "original_dist = np.vstack((x,y,z)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,array):\n",
    "        super().__init__()\n",
    "        self.array = array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.array[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size, dtype=torch.double)]\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append( nn.Linear(hidden_size, hidden_size, dtype=torch.double) )\n",
    "            layers.append( nn.ReLU() )\n",
    "        layers.append( nn.Linear(hidden_size, output_size, dtype=torch.double) )\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class flow_3d(nn.Module):\n",
    "    def __init__(self, pos, hidden_size=128, num_hidden_layers=6):\n",
    "        super(flow_3d, self).__init__()\n",
    "        self.mlp = MLP(3, hidden_size, num_hidden_layers, 3)\n",
    "        if pos == 1: \n",
    "            self.mask = torch.tensor([1,1,0],dtype=torch.double)\n",
    "        elif pos == 2:\n",
    "            self.mask = torch.tensor([1,0,1],dtype=torch.double)\n",
    "        else:\n",
    "            self.mask = torch.tensor([0,1,1],dtype=torch.double)\n",
    "        self.mask = self.mask.view(1,-1)\n",
    "        self.scale_weight1 = nn.Parameter(torch.zeros(1,dtype=torch.double), requires_grad=True)\n",
    "        self.scale_weight2 = nn.Parameter(torch.zeros(1,dtype=torch.double), requires_grad=True)\n",
    "        self.bias_weight = nn.Parameter(torch.zeros(1,dtype=torch.double), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        x_masked = x * self.mask\n",
    "        log_scale1, log_scale2, bias = self.mlp(x_masked).chunk(3,dim=1)\n",
    "        log_scale = log_scale1.tanh() * self.scale_weight1 + log_scale2.tanh() * self.scale_weight2 + self.bias_weight\n",
    "        bias = bias  * (1-self.mask)\n",
    "        log_scale = log_scale * (1-self.mask)\n",
    "        if reverse:\n",
    "            x = (x - bias) * torch.exp(-log_scale)\n",
    "        else:\n",
    "            x = x * torch.exp(log_scale) + bias\n",
    "        return x, log_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model to compose flows and pass results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compose_flow(nn.Module):\n",
    "    def __init__(self, flow_list):\n",
    "        super(compose_flow, self).__init__()\n",
    "        self.flow_list = nn.ModuleList(flow_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, log_det_jacobian = x, torch.zeros_like(x)\n",
    "        for flow in self.flow_list:\n",
    "            z, log_scale = flow(z)\n",
    "            log_det_jacobian += log_scale\n",
    "        return z, log_det_jacobian\n",
    "\n",
    "    def inverse(self, z):\n",
    "        list = []\n",
    "        for flow in self.flow_list[::-1]:\n",
    "            z, _ = flow(z, reverse=True)\n",
    "            list.append(z)\n",
    "        return list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function by negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(target_distribution,z,sum_log_dz_by_dx):\n",
    "    log_likelihood = target_distribution.log_prob(z).view(-1,1) + sum_log_dz_by_dx\n",
    "    return -log_likelihood.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epoch,train_loader,test_loader,target_distribution,lr=0.001):\n",
    "    optimizer = optim.AdamW(flows.parameters(),lr=lr)\n",
    "    model.train()\n",
    "    history = np.zeros((2,epoch))\n",
    "    for i in range(epoch):\n",
    "        epoch_loss = []\n",
    "        for x in train_loader:\n",
    "            z, sum_log_dz_by_dx = model(x)\n",
    "            loss = loss_fn(target_distribution,z,sum_log_dz_by_dx)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_loss = np.mean(epoch_loss)\n",
    "            test_loss = validation(model,test_loader,loss_fn,target_distribution)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Iter: {i+1} Train Loss: {train_loss: .3f} Test Loss: {test_loss: .3f}\")\n",
    "        history[:,i] = (train_loss,test_loss)\n",
    "\n",
    "    torch.save({\"stats\": history}, \"Path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = torch.distributions.MultivariateNormal(torch.tensor([5.,0.,0.],dtype=torch.double), torch.tensor([[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]],dtype=torch.double))\n",
    "g2 = torch.distributions.MultivariateNormal(torch.tensor([-5.,0.,0.],dtype=torch.double), torch.tensor([[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]],dtype=torch.double))\n",
    "train_data = np.concatenate([g1.sample([1024]).numpy(),g2.sample([1024]).numpy()])\n",
    "test_data = np.concatenate([g1.sample([1024]).numpy(),g2.sample([1024]).numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(Dataset(train_data),batch_size=512,shuffle=True)\n",
    "test_loader = data.DataLoader(Dataset(test_data),batch_size=2048,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = torch.distributions.MultivariateNormal(torch.zeros(3), torch.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the flow list and initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_list = [flow_3d(1), flow_3d(2),flow_3d(3), flow_3d(1), flow_3d(2), flow_3d(3), flow_3d(1), flow_3d(2), flow_3d(3)]\n",
    "flows = compose_flow(flows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.eval()\n",
    "X = next(iter(test_loader))\n",
    "z,d = flows(X)\n",
    "\n",
    "compare_3D_graph(train_data,z.detach().numpy(),\"Original distribution\",\"Transform result before Trainning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(flows,1500,train_loader,test_loader,target_distribution,lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_graph(\"Path\",4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.eval()\n",
    "X = next(iter(test_loader))\n",
    "z,d = flows(X)\n",
    "\n",
    "compare_3D_graph(target_distribution.sample([2048]).detach().numpy(),z.detach().numpy(),\"Gaussian\",\"Transform result after trainning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g,t = target_distribution.sample([2048]).detach().numpy(),z.detach().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(2 * 9.6, 2 * 9.6))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax.scatter3D(t.T[0], t.T[1], t.T[2], c = np.abs(t.T[0]) + np.abs(t.T[1]) + np.abs(t.T[2]), cmap =plt.get_cmap('Blues'),alpha=0.5)\n",
    "ax.scatter3D(g.T[0], g.T[1], g.T[2], c = np.abs(g.T[0]) + np.abs(g.T[1]) + np.abs(g.T[2]), cmap =plt.get_cmap('Oranges'),alpha=0.5)\n",
    "plt.title(\"Comparision of two distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_3D_graph(train_data,z.detach().numpy(),\"Original distribution\",\"Transform result after trainning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a graph showing every inverse flow output (Original Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = flows.inverse(z)\n",
    "fig = plt.figure(figsize = (4*4.8,3*4.8))\n",
    "\n",
    "for f in range(len(ls)):\n",
    "    ax = fig.add_subplot(3, 4, f+1, projection='3d')\n",
    "    plt.title(\"The {}th T^-1(Z)\".format(len(ls) - f))\n",
    "\n",
    "    z_ = ls[f].detach().numpy()\n",
    "    ax.scatter3D(z_.T[0], z_.T[1], z_.T[2], c = np.abs(z_.T[0]) + np.abs(z_.T[1]) + np.abs(z_.T[2]), cmap =plt.get_cmap('rainbow_r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a graph showing every inverse flow output (Reverse Generated Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = target_distribution.sample([2048])\n",
    "ls = flows.inverse(g)\n",
    "fig = plt.figure(figsize = (4*4.8,3*4.8))\n",
    "\n",
    "for f in range(len(ls)):\n",
    "    ax = fig.add_subplot(3, 4, f+1, projection='3d')\n",
    "    plt.title(\"The {}th T^-1(Z)\".format(len(ls) - f))\n",
    "\n",
    "    z_ = ls[f].detach().numpy()\n",
    "    ax.scatter3D(z_.T[0], z_.T[1], z_.T[2], c = np.abs(z_.T[0]) + np.abs(z_.T[1]) + np.abs(z_.T[2]), cmap =plt.get_cmap('rainbow_r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Image Flow-based Model Pipeline</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,array):\n",
    "        super().__init__()\n",
    "        self.array = array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.array[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size)]\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append( nn.Linear(hidden_size, hidden_size) )\n",
    "            layers.append( nn.ReLU() )\n",
    "        layers.append( nn.Linear(hidden_size, output_size) )\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class flow_image(nn.Module):\n",
    "\n",
    "    def __init__(self,flows_number=1):\n",
    "        super(flow_image,self).__init__()\n",
    "        self.mlp = MLP(4, 128, 4, 4)\n",
    "        self.flows_number = flows_number\n",
    "        self.scale_weight = nn.Parameter(torch.zeros((4,1)), requires_grad=True)\n",
    "        self.bias_weight = nn.Parameter(torch.zeros((4,1)), requires_grad=True)\n",
    "\n",
    "    def forward(self,X):\n",
    "        if self.flows_number % 2 == 1:\n",
    "            x = torch.chunk(X,2,dim=1)[0]\n",
    "            y = torch.chunk(X,2,dim=1)[1]\n",
    "        else:\n",
    "            y = torch.chunk(X,2,dim=1)[0]\n",
    "            x = torch.chunk(X,2,dim=1)[1]\n",
    "\n",
    "        base = torch.concat([x**0,x**1,x**2,x**3],dim=0).T\n",
    "        self.s = (self.mlp(base).sigmoid() @ self.scale_weight).T\n",
    "        self.b = (self.mlp(base) @ self.bias_weight).T\n",
    "\n",
    "        y_new = self.s.exp() * y + self.b\n",
    "        \n",
    "        if self.flows_number % 2 == 1:\n",
    "            z = torch.concat([x,y_new],dim=1)\n",
    "        else:\n",
    "            z = torch.concat([y_new,x],dim=1)\n",
    "\n",
    "        s = torch.concat([torch.zeros(1,392),self.s],dim=1)\n",
    "\n",
    "        return z, s\n",
    "\n",
    "    def inverse(self,Z):\n",
    "        if self.flows_number % 2 == 1:\n",
    "            x = torch.chunk(Z,2,dim=1)[0]\n",
    "            y = torch.chunk(Z,2,dim=1)[1]\n",
    "        else:\n",
    "            y = torch.chunk(Z,2,dim=1)[0]\n",
    "            x = torch.chunk(Z,2,dim=1)[1]\n",
    "\n",
    "        y_old = (y - self.b) / self.s.exp()\n",
    "\n",
    "        if self.flows_number % 2 == 1:\n",
    "            z = torch.concat([x,y_old],dim=1)\n",
    "        else:\n",
    "            z = torch.concat([y_old,x],dim=1)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model to compose flows and pass results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class composed_flows(nn.Module):\n",
    "    \n",
    "    def __init__(self,models_list):\n",
    "        super(composed_flows,self).__init__()\n",
    "        self.models_list = nn.ModuleList(models_list)\n",
    "\n",
    "    def forward(self,X):\n",
    "        z, sum_log_dz_by_dx = X, 0\n",
    "        for flow in self.models_list:\n",
    "            z, log_dz_by_dx = flow(z)\n",
    "            sum_log_dz_by_dx += log_dz_by_dx\n",
    "        \n",
    "        return z, sum_log_dz_by_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function by negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(target_distribution,z,sum_log_dz_by_dx):\n",
    "    log_likelihood = target_distribution.log_prob(z) + sum_log_dz_by_dx\n",
    "    return -log_likelihood.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epoch,train_loader,target_distribution,lr=0.001):\n",
    "    optimizer = optim.Adam(flows.parameters(),lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        epoch_loss = []\n",
    "        for x in train_loader:\n",
    "            z, sum_log_dz_by_dx = model(x)\n",
    "            loss = loss_fn(target_distribution,z.T,sum_log_dz_by_dx.T)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if i == 0:\n",
    "                print(\"Iter: {} Loss: {:.3f}\".format(i+1,np.mean(epoch_loss)))\n",
    "            elif (i + 1) % 100 == 0:\n",
    "                print(\"Iter: {} Loss: {:.3f}\".format(i+1,np.mean(epoch_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = torch.distributions.Normal(torch.tensor([0.]),torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = torchvision.datasets.MNIST(root=\"/Users/Siyuan/Documents/Learning/Msc Project/Data/mnist\", train=True,transform=torchvision.transforms.ToTensor(), download=True)\n",
    "img = mnist[2][0]\n",
    "\n",
    "noise = target_distribution.sample([784]).T * 0.1\n",
    "train_data = torch.flatten(img,start_dim=1) + noise\n",
    "test_data = torch.flatten(img,start_dim=1) + noise\n",
    "\n",
    "train_loader = data.DataLoader(Dataset(train_data),batch_size=1)\n",
    "test_loader = data.DataLoader(Dataset(test_data),batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the flow list and initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_list = [flow_image(1),flow_image(2),flow_image(1),flow_image(2),flow_image(1),flow_image(2),flow_image(1),flow_image(2),flow_image(1),flow_image(2)]\n",
    "flows = composed_flows(flows_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(flows,1700,train_loader,target_distribution,lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.nn.Unflatten(1,(28,28))(next(iter(train_loader)))\n",
    "flows.eval()\n",
    "z = flows(next(iter(train_loader)))[0]\n",
    "result = torch.nn.Unflatten(1,(28,28))(flows(next(iter(train_loader)))[0])\n",
    "\n",
    "compare_image(img,result,\"Original image\",\"Generated noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a graph showing every inverse flow output (Original Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4*4.8,4*4.8))\n",
    "for f in range(len(flows_list)):\n",
    "    ax = plt.subplot(4,4,f+1)\n",
    "    plt.title(\"After {}th flow's inverse function\".format(len(flows_list) - f))\n",
    "\n",
    "    z = flows_list[len(flows_list) - f - 1].inverse(z)\n",
    "    z_ = torch.nn.Unflatten(1,(28,28))(z)\n",
    "    plt.imshow(np.transpose(z_.detach().numpy(),(1,2,0)),cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a graph showing every inverse flow output (Reverse Generated Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = target_distribution.sample([784]).T\n",
    "fig = plt.figure(figsize = (4*4.8,4*4.8))\n",
    "for f in range(len(flows_list)):\n",
    "    ax = plt.subplot(4,4,f+1)\n",
    "    plt.title(\"After {}th flow's inverse function\".format(len(flows_list) - f))\n",
    "\n",
    "    g = flows_list[len(flows_list) - f - 1].inverse(g)\n",
    "    z_ = torch.nn.Unflatten(1,(28,28))(g)\n",
    "    plt.imshow(np.transpose(z_.detach().numpy(),(1,2,0)),\"gist_gray\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
